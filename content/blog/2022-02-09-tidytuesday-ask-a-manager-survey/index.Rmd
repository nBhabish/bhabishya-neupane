---
title: '#TidyTuesday [Ask a Manager Survey]'
author: Bhabishya Neupane
date: '2022-02-09'
slug: []
excerpt: Predicting salaries based on job title, state, industry, experience in field 
categories:
  - tidytuesday
  - tidymodels
  - machine learning
tags: []
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(tidytuesdayR)
library(lubridate)

update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))

```

### Load the weekly Data

Download the weekly data and make available in the `tt` object.

```{r Load}

tt <- tt_load("2021-05-18")

survey <- tt$survey %>%
  mutate(
    timestamp = mdy_hms(timestamp),
    age_category = fct_reorder(how_old_are_you, parse_number(how_old_are_you)) %>% fct_relevel("under 18")
  ) 

```


### Glimpse Data

Taking an initial look at the format of the data available.

```{r Glimpse}
survey %>% 
  glimpse()
```


### Fixing Data Issues 

```{r}
survey_usd <- survey %>%
  filter(annual_salary >= 5000,
         annual_salary <= 2e6) %>%
  filter(currency == "USD") %>%
  mutate(state = str_remove(state, ", .*")) %>%
  mutate(experience_overall = str_replace(overall_years_of_professional_experience, " - ", "-")) %>%
  mutate(experience_overall = fct_reorder(experience_overall,
                                          parse_number(experience_overall))) %>%
  mutate(experience_in_field = str_replace(years_of_experience_in_field, " - ", "-")) %>%
  mutate(experience_in_field = fct_reorder(experience_in_field, parse_number(experience_in_field))) %>%
  mutate(gender = fct_collapse(
    coalesce(gender, "Other or prefer not to answer"),
    "Other or prefer not to answer" = c("Prefer not to answer", "Other or prefer not to answer")
  )) %>% 
  mutate(race = fct_lump(coalesce(race, "Other"), 4))
  
```



### Exploratory Data Analysis

```{r Visualize}

survey_usd %>% 
  filter(annual_salary >= 5000,
         annual_salary <= 2e6) %>% 
  ggplot(aes(annual_salary))+
  geom_histogram()+
  scale_x_continuous(labels = scales::dollar_format())+
  scale_y_continuous(labels = scales::comma)+
  labs(
    x = "Annual",
    y = "Count",
    title = "Distribution of Annual Salary in the Survey",
    subtitle = "Data was adjusted to account for annual salary greater than or equals to 5,000 and less than $2M")+
  theme_minimal()
  
```

Let's try doing a log 10 transformation and observing what the distribution would look like. 

```{r}
survey_usd %>% 
  filter(annual_salary >= 5000,
         annual_salary <= 2e6) %>% 
  ggplot(aes(annual_salary))+
  geom_histogram()+
  scale_x_log10(labels = scales::dollar_format())+
  scale_y_continuous(labels = scales::comma)+
  labs(
    x = "Annual",
    y = "Count",
    title = "Distribution of Annual Salary in the Survey",
    subtitle = "Data was adjusted to account for annual salary greater than or equals to 5,000 and less than $2M")+
  theme_minimal()
```

We can see that a lot of the people who responded to the survey had an annual salary of $100,000 or somewhere around it for the most part. 

### State EDA

```{r}
survey_usd %>% 
  mutate(state = fct_lump(state, 8),
         state = fct_reorder(state, annual_salary)) %>% 
  ggplot(aes(annual_salary, state))+
  geom_boxplot()+
  scale_x_log10(labels = scales::dollar_format())
```


#### Quick Median Check

```{r}

summarise_salary <- function(tbl){
  tbl %>% 
    summarise(n = n(),
              median_salary = median(annual_salary))
}


survey_usd %>% 
  filter(!is.na(state)) %>% 
  mutate(state = fct_lump(state, 9),
         state = fct_reorder(state, annual_salary)) %>% 
  group_by(state) %>% 
  summarise_salary()%>% 
  arrange(desc(n)) %>% 
  ggplot(aes(state, median_salary))+
  geom_col()+
  coord_flip()+
  scale_y_continuous(labels = scales::dollar)
  

```



### Industry EDA

```{r}
survey_usd %>% 
  filter(!is.na(industry)) %>% 
  mutate(industry = fct_lump(industry, 9),
         industry = fct_reorder(industry, annual_salary)) %>% 
  group_by(industry) %>% 
  summarise_salary()%>% 
  arrange(desc(n)) %>% 
  ggplot(aes(industry, median_salary))+
  geom_col()+
  coord_flip()+
  scale_y_continuous(labels = scales::dollar)
```


### Customized Function

I am going to use the same code multiple times, so I think the best approach would be to create a function to modularize the code.

```{r}
plot_categorical <- function(tbl, column, n_levels = 9, reorder = TRUE) {
  tbl_lumped <- tbl %>% 
    filter(!is.na({{ column }})) %>% 
    mutate({{ column }} := fct_lump({{ column }}, n_levels))
  
  if(reorder) {
    
    tbl_lumped <- tbl_lumped %>% 
      mutate({{ column }} := fct_reorder({{ column }}, annual_salary))
  }
  
  tbl_lumped %>% 
    group_by({{ column }}) %>% 
    summarise_salary() %>% 
    ggplot(aes(median_salary, {{ column }}))+
    geom_col()+
    scale_x_continuous(labels = scales::dollar_format())+
    labs(x = "Median Salary")
}
```


### Job Title by Median Salary

```{r}
survey_usd %>% 
  plot_categorical(job_title, n_levels = 15)+
  labs(y = "Job Title")
```


```{r}

survey_usd %>% 
  plot_categorical(experience_overall, reorder = FALSE)
```


### Statistical Inferences and ANOVA 

- ANOVA stands for analysis of variance. It tries looking at the variance within groups and compares the variance between groups

```{r}
survey_usd %>% 
  filter(!is.na(experience_overall)) %>% 
  ggplot(aes(annual_salary, experience_overall))+
  geom_boxplot()+
  scale_x_log10(labels = scales::dollar_format())
```


```{r}
library(broom)
lm(log2(annual_salary) ~ experience_overall, data = survey_usd) %>% 
  summary()

lm(log2(annual_salary) ~ experience_in_field, data = survey_usd) %>%
  summary() 
```


- R-Square represents how much of the variation in `annual_salary` is explained by `experience_overall`. 

- `experience_in_field` explains more variation compared to `experience_overall`.

- From our summary statistics, `experience_in_field` is a better predictor than `experience_overall` since the former was able to explain 10.73% of the variation in the data. 

### Gender

```{r}
survey_usd %>% 
  plot_categorical(gender)

lm(log2(annual_salary) ~ gender, data = survey_usd) %>% 
  summary()
```

### Race 

```{r}
survey_usd %>% 
  plot_categorical(race, n_levels = 4)
```


```{r}
survey_usd %>% 
  mutate(job_title = fct_lump(job_title, 10)) %>% 
  lm(log2(annual_salary) ~ job_title, data = .) %>%
  summary() 
```

- `job_title` explains very little about the variation in the `annual_salary`, or at least explains less than `experience_in_field` and `experience_overall`.


### How to make log2 interpretable?

```{r}

survey_usd %>% 
  plot_categorical(job_title, reorder = TRUE, n_levels = 15)
```


```{r}
survey %>% 
  mutate(job_title = fct_lump(job_title, 10)) %>% 
  lm(log2(annual_salary) ~ job_title , data = .) %>%
  summary()
```

The intercept value represents the estimate for Administrative Assistant, and what does the other term mean? A one on the rest of the estimates implies twice the salary to of an Administrative Assistant. A two on the scale would mean four times the salary to of an Administrative Assistant. 

Okay, let's try explaining the estimate of Program Manager and Project Manager. We can see the estimates for Program Manager and Project Manager are 0.926 and 0.906. Let's say pretty close to 1. That basically implies the annual salary of Program or Project Manager is twice the salary of an Administrative Assistant. 

We can just about confirm that from the Job Title vs Median Salary Plot.



```{r}
survey_usd %>% 
  mutate(job_title = fct_lump(job_title, 10)) %>% 
  mutate(state = fct_lump(state, 10),
         industry = fct_lump(industry, 10)) %>% 
  lm(log2(annual_salary) ~ job_title + state + industry + experience_in_field + race + gender, data = .) %>%
  anova() %>% 
  tidy() %>% 
  mutate(pct_of_variation = sumsq/sum(sumsq)) %>% 
  arrange(desc(pct_of_variation))
```


### Predictive Modeling/ Machine Learning 

```{r}
library(tidymodels)

set.seed(2022)
survey_usd_split <- survey_usd %>% 
  initial_split()

survey_usd_training <- training(survey_usd_split)
survey_usd_testing <- testing(survey_usd_split)
```

### Preparing recipe

```{r}
recipe <- survey_usd_training %>%
  recipe(
    annual_salary ~ job_title + state + experience_in_field + industry + race + gender + highest_level_of_education_completed
  ) %>%
  step_novel(all_nominal()) %>%
  step_unknown(job_title,
               industry,
               state,
               highest_level_of_education_completed,
               gender) %>%
  step_log(annual_salary, base = 2) %>%
  step_other(job_title, industry, state, threshold = tune()) %>%
  #Anything that's rare than 0.05 will be collapsed into Other for predictors mentioned above
  step_dummy(all_nominal())
```


### Cross Validation

```{r}
training_cv <- vfold_cv(survey_usd_training) 
```


```{r}

threshold_grid <- crossing(threshold = c(0.001, .003, .01, .03, .1))

linear_model_tune_cv_threshold <- linear_reg() %>%
  set_engine("lm") %>%
  tune_grid(recipe,
            training_cv,
            grid = threshold_grid)
```


```{r}
linear_model_tune_cv_threshold %>% 
  collect_metrics() %>% 
  ggplot(aes(threshold, mean, color = .metric))+
  geom_line()+
  scale_x_log10()
```


```{r}
recipe_with_threshold <- recipe %>% 
  finalize_recipe(list(threshold = 0.001))
```


```{r}
linear_model_cv <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit_resamples(recipe_with_threshold, training_cv)

linear_model_cv %>% 
  collect_metrics()
```


### Random Forest 


```{r}
random_forest_tuning <- rand_forest(mode = "regression", 
                                mtry  = tune(),
                                trees = tune()) %>% 
  set_engine("ranger") %>% 
  tune_bayes(recipe_with_threshold, training_cv)
```





```{r}
recipe %>% 
  prep() %>% 
  juice() %>% 
  skimr::skim()
```












